# crawler
Поиск файлов на сайте https://www.softpedia.com.

<br/>
<br/>

Основная идея в том, что страницы можно разделить на 3 группы:
1) страницы со списокм файлов (такие как https://win.softpedia.com/).
2) страницы конкретных файлов (такие как https://linux.softpedia.com/get/System/Operating-Systems/Linux-Distributions/SparkyLinux-Ultra-102961.shtml).
3) popup, встраиваемый в страницы из пункта 2, при нажатии на кнопку "загрузить файл".

<br/>
<br/>

Соответственно, чтобы получить ссылки на загрузку файлов, нам нужно рекурсивно пройти всю пагинацию страниц из пункта 1,
собирая ссылки на страницы из пункта 2.
 
Далее нужно запросить каждую страницу из пункта 2 и сформировать на её основе POST запрос на получение popup-а из пункта 3.

После чего уже обрабатывать mirror ссылки на сам файл.

<br/>
<br/>

На данный момент не реализованы:
1) загрузка и анализ содержимого архивов.
2) retry-и при получении ошибки загрузки html страницы.
3) нормальная логика конфигурирования приложения.
4) сохранение результата в каком-нибудь адекватном формате (навроде json-а).
5) тротлинг запросов. Даже если запустить всё с паралелизмом равным единице, сайт очень быстро начинает банить
 (т.к. нет тротлинга хотябы в случае ошибки загрузки страницы). В ответ на запрос страницы файла (из пункта 2) приходит
  пустой html.
